# All local jobs are part of the vanilla universe.
Universe       = vanilla

# We want email if the job completed successfully. This can
# be set to Always, Error, or Never.
Notification   = Never

PeriodicHold   = (NumJobStarts>=1 && JobStatus == 1)

# Jobs by default get 1.4Gb of RAM allocated, ask for more if needed
# but if a job needs more than 2Gb it will not be able to run on the
# older nodes
request_memory = 8GB

# If you need multiple cores you can ask for them, but the scheduling
# may take longer the "larger" a job you ask for
request_cpus=1

# This flag is used to order only one's own submitted jobs 
# The jobs with the highest numbers get considered for 
# scheduling first.
Priority=20

# Copy all of the user's current shell environment variables 
# at the time of job submission.
GetEnv=True

# The requirement line specifies which machines we want to
# run this job on.  Any arbitrary classad expression can
# be used.
Requirements=(CPU_Speed >= 1)

# Rank is an expression that states how to rank machines which 
# have already met the requirements expression.  Essentially, 
# rank expresses preference.  A higher numeric value equals better 
# rank.  Condor will give the job the machine with the highest rank.
Rank=CPU_Speed

# Used to give jobs a directory with respect to file input 
# and output.
Initialdir     = /sphenix/u/xyu3/workarea/PhotonConv/JpsiReco

# The executable we want to run.
Executable     = $(Initialdir)/runHFreco.sh

Nevent         = 10000
RunNumber      = 53877
ClusterFolder  = ana494_2024p021_v001
TrackFolder    = ana495_2024p021_v001
#CaloFolder     = ana468_2024p012_v001
CaloFolder     = new_newcdbtag_v006
RunRange       = run_00053800_00053900
ClusterFileName= DST_TRKR_CLUSTER_run2pp_$(ClusterFolder)-$INT(RunNumber,%08d)-$INT(TrkrSegment,%05d).root
ClusterFilePath= /sphenix/lustre01/sphnxpro/production/run2pp/physics/$(ClusterFolder)/DST_TRKR_CLUSTER/$(RunRange)/dst/
TrackFileName  = DST_TRKR_TRACKS_run2pp_$(TrackFolder)-$INT(RunNumber,%08d)-$INT(TrkrSegment,%05d).root
TrackFilePath  = /sphenix/lustre01/sphnxpro/production/run2pp/physics/$(TrackFolder)/DST_TRKR_TRACKS/$(RunRange)/dst/
#CaloFileName   = DST_CALO_run2pp_$(CaloFolder)-$INT(RunNumber,%08d)-$INT(CaloSegment,%05d).root
#CaloFilePath   = /sphenix/lustre01/sphnxpro/production/run2pp/physics/$(CaloFolder)/DST_CALO/$(RunRange)/dst/
CaloFileName   = DST_CALOFITTING_run2pp_$(CaloFolder)-$INT(RunNumber,%08d)-$INT(CaloSegment,%05d).root
CaloFilePath   = /sphenix/lustre01/sphnxpro/production2/run2pp/physics/caloy2fitting/$(CaloFolder)/$(RunRange)/
OutPrefix      = outputKFParticle_
OutPath        = /sphenix/u/xyu3/hftg01/Quarkonium_productions/
Index          = 0
StepSize       = 0

# The argument to pass to the executable.
#tracking firt then calo, only one tracking file
Arguments      = "$(Nevent) $(ClusterFileName) $(ClusterFilePath) $(TrackFileName) $(TrackFilePath) $(CaloFileName) $(CaloFilePath) $(OutPrefix) $(OutPath) $(Index) $(StepSize)"

# The job's stdout is sent to this file.
Output         = $(Initialdir)/log/job-Data-$(RunNumber).$(TrkrSegment).$(Cluster).$(Process).out

# The job's stderr is sent to this file.
Error          = $(Initialdir)/log/job-Data-$(RunNumber).$(TrkrSegment).$(Cluster).$(Process).err

# The condor log file for this job, useful when debugging.
Log            = $(Initialdir)/log/job-Data-$(RunNumber).$(TrkrSegment).$(Cluster).$(Process).log

#should_transfer_files   = YES
#when_to_transfer_output = ON_EXIT_OR_EVICT
#transfer_output_files   = dummy.cc

on_exit_hold = (ExitBySignal == True) || (ExitCode != 0)

#Limiting the number of running jobs
#concurrency_limits=CONCURRENCY_LIMIT_DEFAULT:1000

# This should be the last command and tells condor to queue the
# job.  If a number is placed after the command (i.e. Queue 15)
# then the job will be submitted N times.  Use the $(Process)
# macro to make your input/output and log files unique.
Queue TrkrSegment CaloSegment from $(Initialdir)/filelist/dst_sync_trkr_calo_run2pp-$INT(RunNumber,%08d).list
